#Laboratorio 8 - SIS420
#Nombre: Escobar Ruiz Marco Antonio - Ing Sistemas
import time  # Se usa para pausar entre pasos y ralentizar la simulaci贸n
from stable_baselines3 import PPO  # Importamos el algoritmo entrenado PPO
from flip_cup_env import FlipCupEnv  # Importamos el entorno personalizado que creaste
import pybullet as p


# Cargamos el modelo previamente entrenado desde el archivo .zip
model = PPO.load("ppo_flipcup_policy")

#  Creamos una instancia del entorno con renderizado activado
# Esto abre la ventana de simulaci贸n visual para observar el comportamiento del agente
env = FlipCupEnv(render=True)
# Reiniciamos el entorno para comenzar un nuevo episodio
obs = env.reset()


# Marcador que indica si el episodio termin贸
done = False

# Contador de recompensa total obtenida por el agente en este episodio
episode_reward = 0

# El agente toma una acci贸n paso a paso hasta que el entorno indique que termin贸
while not done:
    # El modelo predice la mejor acci贸n posible dado el estado actual (obs)
    # deterministic=True evita aleatoriedad: muestra c贸mo se comporta el agente "en serio"
    action, _ = model.predict(obs, deterministic=True)
    print(f"Action: {action}")
    # Aplicamos esa acci贸n al entorno, que nos devuelve:
    # - la siguiente observaci贸n
    # - la recompensa obtenida
    # - si el episodio termin贸
    # - un diccionario adicional vac铆o por ahora
    obs, reward, done, _ = env.step(action)

    # Acumulamos la recompensa total del episodio
    episode_reward += reward

    # Imprimimos la recompensa obtenida en este paso para observar el progreso

    print(f"Reward: {reward:.2f}")

    # Ralentizamos la simulaci贸n para que el movimiento se vea claramente (~20 FPS)
    time.sleep(0.05)
env.close()
print(f"\n Recompensa total final del episodio: {episode_reward:.2f}")